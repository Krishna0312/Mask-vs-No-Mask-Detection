{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Real_time_mask_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewbW3-AeGZ5c"
      },
      "source": [
        "# 1. Data preprocessing & Model building\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq7U1rLMo3XV"
      },
      "source": [
        "!pip install keras-tuner -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX17UOyq5vtz"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88YThLt7KvVG",
        "outputId": "beb3a9d0-282e-450f-d42b-64c03c23669a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkXnFHuwDAfK"
      },
      "source": [
        "#!wget --no-check-certificate \\\n",
        "#    \"https://github.com/prajnasb/obatch_sizeervations/archive/refs/heads/master.zip\" \\\n",
        "#    -O \"/tmp/obatch_sizeervations.zip\"\n",
        "\n",
        "\n",
        "#zip_ref = zipfile.ZipFile('/tmp/obatch_sizeervations.zip', 'r') #Opens the zip file in read mode\n",
        "#zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "#zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4VAKCed57bA",
        "outputId": "2fc48321-1013-4747-8113-7dbc7eca8bbf"
      },
      "source": [
        "lr = 1e-3\n",
        "epoch = 30 #25,35,40\n",
        "batch_size = 16 \n",
        "\n",
        "DIRECTORY = \"/content/drive/My Drive/FaceMaskDetection-main/FaceMaskDetection-main/data/data/\" #C:\\Python\\FaceMaskDetection-main\\FaceMaskDetection-main\\data\n",
        "TEST = \"/content/drive/My Drive/face_mask_test/test_images/\"\n",
        "CATEGORIES = [\"with_mask\", \"without_mask\"]\n",
        "\n",
        "# grab the list of images in our dataset directory, then initialize\n",
        "# the list of data (i.e., images) and class images\n",
        "print(\"[INFO] loading images...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrdK6GNs57b-"
      },
      "source": [
        "data = []\n",
        "labels = []\n",
        "\n",
        "for category in CATEGORIES:\n",
        "    path = os.path.join(DIRECTORY, category)\n",
        "    for img in os.listdir(path):\n",
        "    \timg_path = os.path.join(path, img)\n",
        "    \timage = load_img(img_path, target_size=(224, 224))    #299, 299 : inceptionresnetv2\n",
        "    \timage = img_to_array(image)\n",
        "    \timage = preprocess_input(image)\n",
        "\n",
        "    \tdata.append(image)\n",
        "    \tlabels.append(category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdEGMMz_57gA"
      },
      "source": [
        "# perform one-hot encoding on the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "labels = np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17I5-f-N57g3"
      },
      "source": [
        "(x_train, x_test, y_train, y_test) = train_test_split(data, labels,\n",
        "\ttest_size=0.10, stratify=labels, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWDUN_oL57la"
      },
      "source": [
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(\n",
        "\trotation_range=20,\n",
        "\tzoom_range=0.15,\n",
        "\twidth_shift_range=0.2,\n",
        "\theight_shift_range=0.2,\n",
        "\tshear_range=0.15,\n",
        "\thorizontal_flip=True,\n",
        "  vertical_flip = True,\n",
        "\tfill_mode=\"nearest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Riw72E-Y57zD",
        "outputId": "5d1bdde0-d41d-4bbf-f020-d3b69de28e31"
      },
      "source": [
        "# load the MobileNetV2 network, ensuring the head FC layer sets are\n",
        "# left off\n",
        "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "\n",
        "#baseModel = InceptionResNetV2(include_top=False, weights=\"imagenet\", input_tensor=Input(shape=(299, 299, 3)))\n",
        "\n",
        "#baseModel = tf.keras.applications.InceptionV3(include_top=False, weights=\"imagenet\", input_tensor=Input(shape=(299,299,3)), classifier_activation=\"softmax\") #classes=1000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkOK9GHO6ltL"
      },
      "source": [
        "\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.3)(headModel)  \n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE2beglW6lt-"
      },
      "source": [
        "# freezw the base model during training\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False            #true: fine tuning whole model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho4ayFP4Hdwd"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "      if epoch < 10:\n",
        "        return lr\n",
        "      else:\n",
        "        return lr * tf.math.exp(-0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK9m_xLv6lx4"
      },
      "source": [
        "# compile our model\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=0)\n",
        "opt = Adam(learning_rate=lr, decay=lr / epoch)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ_6Ps0U2ayd"
      },
      "source": [
        "# 1.1 Keras Tuner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ6eoF_Kopgl"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from keras_tuner import RandomSearch\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    baseModel = MobileNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "    for layer in baseModel.layers:\n",
        "\t    layer.trainable = False\n",
        "    headModel = baseModel.output\n",
        "    headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "    headModel = Flatten(name=\"flatten\")(headModel)\n",
        "    headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "    headModel = Dropout(hp.Choice(\"rate\", values=[0.3,0.5]))(headModel)  \n",
        "    headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "\n",
        "    model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "  \n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(hp.Choice(\"learning_rate\", values=[4e-4, 1e-3, 1e-4])), metrics=[\"accuracy\"])      \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "350BwJl5ow_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98feeaf4-dac0-4ff3-a789-866e2fa5641b"
      },
      "source": [
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=3,\n",
        "    executions_per_trial=2,\n",
        "    overwrite=True) #directory=\"my_dir\",project_name=\"helloworld\","
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdRAwRIHopjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff08394f-b58f-4ec8-9bea-4802fc944a22"
      },
      "source": [
        "tuner.search(x_train, y_train, epochs=2, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 02m 42s]\n",
            "val_accuracy: 0.97826087474823\n",
            "\n",
            "Best val_accuracy So Far: 1.0\n",
            "Total elapsed time: 00h 08m 20s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9ryuIG4z0th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29eb5b0a-3442-4438-c469-3fcb17624306"
      },
      "source": [
        "models = tuner.get_best_models(num_models=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpAjyzHkz0vH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6024e43f-591c-410f-9b1f-5e3fc4f3088d"
      },
      "source": [
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./untitled_project\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "rate: 0.3\n",
            "learning_rate: 0.0004\n",
            "Score: 1.0\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "rate: 0.3\n",
            "learning_rate: 0.001\n",
            "Score: 1.0\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "rate: 0.3\n",
            "learning_rate: 0.0001\n",
            "Score: 0.97826087474823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpvupN8H2E14"
      },
      "source": [
        "# 2. Model training after optimization of hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2s2zTDIBbPF"
      },
      "source": [
        "<font size =5, font color ='yellow'> a) MobileNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI0Y6IVF8icB"
      },
      "source": [
        "Best value: lr = 0.01 dropout rate = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcazzQze6lyj",
        "outputId": "4ec7e639-06f6-4f27-ce33-f29db8a644d2"
      },
      "source": [
        "# train the head of the network\n",
        "\n",
        "H = model.fit(aug.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=len(x_train) // batch_size, validation_data=(x_test, y_test), validation_steps=len(x_test) // batch_size, callbacks=[callback],\n",
        "\tepochs=epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "77/77 [==============================] - 52s 635ms/step - loss: 0.1622 - accuracy: 0.9345 - val_loss: 0.0253 - val_accuracy: 0.9928\n",
            "Epoch 2/30\n",
            "77/77 [==============================] - 48s 622ms/step - loss: 0.0424 - accuracy: 0.9894 - val_loss: 0.0390 - val_accuracy: 0.9783\n",
            "Epoch 3/30\n",
            "77/77 [==============================] - 48s 623ms/step - loss: 0.0448 - accuracy: 0.9836 - val_loss: 0.0270 - val_accuracy: 0.9855\n",
            "Epoch 4/30\n",
            "77/77 [==============================] - 48s 624ms/step - loss: 0.0252 - accuracy: 0.9926 - val_loss: 0.0138 - val_accuracy: 0.9928\n",
            "Epoch 5/30\n",
            "77/77 [==============================] - 48s 625ms/step - loss: 0.0290 - accuracy: 0.9910 - val_loss: 0.0249 - val_accuracy: 0.9928\n",
            "Epoch 6/30\n",
            "77/77 [==============================] - 48s 625ms/step - loss: 0.0177 - accuracy: 0.9943 - val_loss: 0.0339 - val_accuracy: 0.9855\n",
            "Epoch 7/30\n",
            "77/77 [==============================] - 48s 625ms/step - loss: 0.0186 - accuracy: 0.9935 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "Epoch 8/30\n",
            "77/77 [==============================] - 48s 626ms/step - loss: 0.0258 - accuracy: 0.9926 - val_loss: 0.0158 - val_accuracy: 0.9928\n",
            "Epoch 9/30\n",
            "77/77 [==============================] - 48s 623ms/step - loss: 0.0230 - accuracy: 0.9935 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
            "Epoch 10/30\n",
            "77/77 [==============================] - 48s 624ms/step - loss: 0.0265 - accuracy: 0.9869 - val_loss: 0.0340 - val_accuracy: 0.9855\n",
            "Epoch 11/30\n",
            "77/77 [==============================] - 48s 624ms/step - loss: 0.0546 - accuracy: 0.9853 - val_loss: 0.0185 - val_accuracy: 0.9928\n",
            "Epoch 12/30\n",
            "77/77 [==============================] - 48s 624ms/step - loss: 0.0227 - accuracy: 0.9918 - val_loss: 0.0154 - val_accuracy: 0.9928\n",
            "Epoch 13/30\n",
            "77/77 [==============================] - 48s 623ms/step - loss: 0.0183 - accuracy: 0.9926 - val_loss: 0.0215 - val_accuracy: 0.9928\n",
            "Epoch 14/30\n",
            "77/77 [==============================] - 48s 624ms/step - loss: 0.0065 - accuracy: 0.9992 - val_loss: 0.0194 - val_accuracy: 0.9928\n",
            "Epoch 15/30\n",
            "77/77 [==============================] - 48s 625ms/step - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.0161 - val_accuracy: 0.9928\n",
            "Epoch 16/30\n",
            "77/77 [==============================] - 48s 624ms/step - loss: 0.0056 - accuracy: 0.9992 - val_loss: 0.0253 - val_accuracy: 0.9855\n",
            "Epoch 17/30\n",
            "77/77 [==============================] - 48s 624ms/step - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.0170 - val_accuracy: 0.9928\n",
            "Epoch 18/30\n",
            "77/77 [==============================] - 48s 623ms/step - loss: 0.0150 - accuracy: 0.9943 - val_loss: 0.0208 - val_accuracy: 0.9928\n",
            "Epoch 19/30\n",
            "77/77 [==============================] - 48s 622ms/step - loss: 0.0099 - accuracy: 0.9967 - val_loss: 0.0143 - val_accuracy: 0.9928\n",
            "Epoch 20/30\n",
            "77/77 [==============================] - 48s 624ms/step - loss: 0.0156 - accuracy: 0.9959 - val_loss: 0.0203 - val_accuracy: 0.9855\n",
            "Epoch 21/30\n",
            "77/77 [==============================] - 48s 621ms/step - loss: 0.0185 - accuracy: 0.9935 - val_loss: 0.0368 - val_accuracy: 0.9855\n",
            "Epoch 22/30\n",
            "77/77 [==============================] - 48s 622ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0188 - val_accuracy: 0.9928\n",
            "Epoch 23/30\n",
            "77/77 [==============================] - 48s 623ms/step - loss: 0.0087 - accuracy: 0.9959 - val_loss: 0.0135 - val_accuracy: 0.9928\n",
            "Epoch 24/30\n",
            "77/77 [==============================] - 48s 625ms/step - loss: 0.0042 - accuracy: 0.9984 - val_loss: 0.0166 - val_accuracy: 0.9928\n",
            "Epoch 25/30\n",
            "77/77 [==============================] - 48s 626ms/step - loss: 0.0053 - accuracy: 0.9975 - val_loss: 0.0270 - val_accuracy: 0.9855\n",
            "Epoch 26/30\n",
            "77/77 [==============================] - 48s 623ms/step - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.0168 - val_accuracy: 0.9928\n",
            "Epoch 27/30\n",
            "77/77 [==============================] - 48s 624ms/step - loss: 0.0052 - accuracy: 0.9992 - val_loss: 0.0310 - val_accuracy: 0.9855\n",
            "Epoch 28/30\n",
            "77/77 [==============================] - 48s 624ms/step - loss: 0.0089 - accuracy: 0.9959 - val_loss: 0.0198 - val_accuracy: 0.9855\n",
            "Epoch 29/30\n",
            "77/77 [==============================] - 48s 624ms/step - loss: 0.0047 - accuracy: 0.9975 - val_loss: 0.0376 - val_accuracy: 0.9855\n",
            "Epoch 30/30\n",
            "77/77 [==============================] - 48s 626ms/step - loss: 0.0061 - accuracy: 0.9984 - val_loss: 0.0185 - val_accuracy: 0.9855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxN3MAbX8JXd"
      },
      "source": [
        "<font size =5, font color ='pink'> Model testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjfUPyR8NX2r"
      },
      "source": [
        "tdata = []\n",
        "tlabels = []\n",
        "\n",
        "for category in CATEGORIES:\n",
        "    path = os.path.join(TEST, category)\n",
        "    for img in os.listdir(path):\n",
        "    \timg_path = os.path.join(path, img)\n",
        "    \timage = load_img(img_path, target_size=(224, 224))  #299, 299 : inceptionresnetv2\n",
        "    \timage = img_to_array(image)\n",
        "    \timage = preprocess_input(image)\n",
        "\n",
        "    \ttdata.append(image)\n",
        "    \ttlabels.append(category)\n",
        "     \n",
        "\n",
        "     \n",
        "     #x_test is working!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_3JbVgU6l3j"
      },
      "source": [
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(tdata, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADGsjkYi6_CU"
      },
      "source": [
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S0SUPIYi4ro"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(tlabels, predIdxs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pDCD-UQjNXz"
      },
      "source": [
        "print(\"Test accuracy: \", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auUII_In6_DW"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(y_test.argmax(axis=1), predIdxs,\n",
        "\ttarget_names=lb.classes_))\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving mask detector model...\")\n",
        "save_path = os.path.join(DIRECTORY, \"mobilenet/\")\n",
        "model.save(\"face_mask_detector1.model\", save_path) #tf.saved_model.save(model, save_path)\n",
        "#model.save(\"face_mask_detector.model\", save_format=\"h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "bXrjCc4N6_Hi",
        "outputId": "32c2db53-4b71-4007-9ee3-27f0e6ef09a6"
      },
      "source": [
        "# plot the training loss and accuracy\n",
        "N = epoch\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"plot.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkyYlv_C-0Zi"
      },
      "source": [
        "# <font size =5, font color ='yellow'> b) InceptionResNetV2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpTKCO6X-nqa"
      },
      "source": [
        "bm = InceptionResNetV2(include_top=False, weights=\"imagenet\", input_tensor=Input(shape=(299, 299, 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWpcAMwD_TFR"
      },
      "source": [
        "\n",
        "hm = bm.output\n",
        "hm = AveragePooling2D(pool_size=(7, 7))(hm)\n",
        "hm = Flatten(name=\"flatten\")(hm)\n",
        "hm = Dense(128, activation=\"relu\")(hm)\n",
        "hm = Dropout(0.3)(hm)  \n",
        "hm = Dense(2, activation=\"softmax\")(hm)\n",
        "\n",
        "\n",
        "model2 = Model(inputs=bm.input, outputs=hm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ulTzZgC_auC"
      },
      "source": [
        "# freezw the base model during training\n",
        "for layer in bm.layers:\n",
        "\tlayer.trainable = False            #true: fine tuning whole model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32UDdtt1_gUk"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "      if epoch < 10:\n",
        "        return lr\n",
        "      else:\n",
        "        return lr * tf.math.exp(-0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru_84P8L_nKR"
      },
      "source": [
        "# compile our model\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=0)\n",
        "opt = Adam(learning_rate=lr, decay=lr / epoch)\n",
        "model2.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTSSYbsXEjn_"
      },
      "source": [
        "# train the head of the network\n",
        "\n",
        "H = model.fit(aug.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=len(x_train) // batch_size, validation_data=(x_test, y_test), validation_steps=len(x_test) // batch_size, callbacks=[callback],\n",
        "\tepochs=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHiiwgFrCPF8"
      },
      "source": [
        "<font size =5, font color ='pink'> Model testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deqwo6bWAe8J"
      },
      "source": [
        "tdata = []\n",
        "tlabels = []\n",
        "\n",
        "for category in CATEGORIES:\n",
        "    path = os.path.join(TEST, category)\n",
        "    for img in os.listdir(path):\n",
        "    \timg_path = os.path.join(path, img)\n",
        "    \timage = load_img(img_path, target_size=(224, 224))  #299, 299 : inceptionresnetv2\n",
        "    \timage = img_to_array(image)\n",
        "    \timage = preprocess_input(image)\n",
        "\n",
        "    \ttdata.append(image)\n",
        "    \ttlabels.append(category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir0Qt8xtAkmk"
      },
      "source": [
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(tdata, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiKQFPYFArSM"
      },
      "source": [
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0bXkPQYAyjv"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(y_test.argmax(axis=1), predIdxs,\n",
        "\ttarget_names=lb.classes_))\n",
        "\n",
        "save_path = os.path.join(DIRECTORY, \"incepresnet/\")\n",
        "model.save(\"face_mask_detector2.model\", save_path)\n",
        "#model.save(\"face_mask_detector.model\", save_format=\"h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ1WyXYb-xVd"
      },
      "source": [
        "# <font size =5, font color ='yellow'> c) InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzKmmMht-sDJ"
      },
      "source": [
        "baseModel = tf.keras.applications.InceptionV3(include_top=False, weights=\"imagenet\", input_tensor=Input(shape=(299,299,3)), classifier_activation=\"softmax\") #classes=1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTK3ej7f_Tyx"
      },
      "source": [
        "\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.3)(headModel)  \n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4yfWukT_b40"
      },
      "source": [
        "# freezw the base model during training\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False            #true: fine tuning whole model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdQkLyBX_hfd"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "      if epoch < 10:\n",
        "        return lr\n",
        "      else:\n",
        "        return lr * tf.math.exp(-0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF4wUxkJ_o63"
      },
      "source": [
        "# compile our model\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=0)\n",
        "opt = Adam(learning_rate=lr, decay=lr / epoch)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvebRD0WErPS"
      },
      "source": [
        "# train the head of the network\n",
        "\n",
        "H = model.fit(aug.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=len(x_train) // batch_size, validation_data=(x_test, y_test), validation_steps=len(x_test) // batch_size, callbacks=[callback],\n",
        "\tepochs=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnMI019vCUKn"
      },
      "source": [
        "<font size =5, font color ='pink'> Model testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyLHA-_aAgMW"
      },
      "source": [
        "tdata = []\n",
        "tlabels = []\n",
        "\n",
        "for category in CATEGORIES:\n",
        "    path = os.path.join(TEST, category)\n",
        "    for img in os.listdir(path):\n",
        "    \timg_path = os.path.join(path, img)\n",
        "    \timage = load_img(img_path, target_size=(224, 224))  #299, 299 : inceptionresnetv2\n",
        "    \timage = img_to_array(image)\n",
        "    \timage = preprocess_input(image)\n",
        "\n",
        "    \ttdata.append(image)\n",
        "    \ttlabels.append(category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgFxEeCbAjJf"
      },
      "source": [
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(tdata, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeqgbQWcAqJl"
      },
      "source": [
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBMU_YjVAuiU"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(y_test.argmax(axis=1), predIdxs,\n",
        "\ttarget_names=lb.classes_))\n",
        "\n",
        "save_path = os.path.join(DIRECTORY, \"incept/\")\n",
        "model.save(\"face_mask_detector3.model\", save_path)\n",
        "#model.save(\"face_mask_detector.model\", save_format=\"h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rx20L5BGn_V"
      },
      "source": [
        "# 3. Testing model on real-time data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGmJrqNvGyGA"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from imutils.video import VideoStream\n",
        "import numpy as np\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFaz8RiqGyKP"
      },
      "source": [
        "\n",
        "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
        "\t# grab the dimensions of the frame and then construct a blob\n",
        "\t# from it\n",
        "\t(h, w) = frame.shape[:2]\n",
        "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (224, 224),\n",
        "\t\t(104.0, 177.0, 123.0))\n",
        "\n",
        "\t# pass the blob through the network and obtain the face detections\n",
        "\tfaceNet.setInput(blob)\n",
        "\tdetections = faceNet.forward()\n",
        "\tprint(detections.shape)\n",
        "\n",
        "\t# initialize our list of faces, their corresponding locations,\n",
        "\t# and the list of predictions from our face mask network\n",
        "\tfaces = []\n",
        "\tlocs = []\n",
        "\tpreds = []\n",
        "\n",
        "\t# loop over the detections\n",
        "\tfor i in range(0, detections.shape[2]):\n",
        "\t\t# extract the confidence (i.e., probability) associated with\n",
        "\t\t# the detection\n",
        "\t\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t\t# filter out weak detections by ensuring the confidence is\n",
        "\t\t# greater than the minimum confidence\n",
        "\t\tif confidence > 0.5:\n",
        "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t\t# the object\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t# ensure the bounding boxes fall within the dimensions of\n",
        "\t\t\t# the frame\n",
        "\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
        "\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "\n",
        "\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
        "\t\t\t# ordering, resize it to 224x224, and preprocess it\n",
        "\t\t\tface = frame[startY:endY, startX:endX]\n",
        "\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "\t\t\tface = cv2.resize(face, (224, 224))\n",
        "\t\t\tface = img_to_array(face)\n",
        "\t\t\tface = preprocess_input(face)\n",
        "\n",
        "\t\t\t# add the face and bounding boxes to their respective\n",
        "\t\t\t# lists\n",
        "\t\t\tfaces.append(face)\n",
        "\t\t\tlocs.append((startX, startY, endX, endY))\n",
        "            \n",
        "            \n",
        "            \n",
        "\n",
        "\t# only make a predictions if at least one face was detected\n",
        "\tif len(faces) > 0:\n",
        "\t\t# for faster inference we'll make batch predictions on *all*\n",
        "\t\t# faces at the same time rather than one-by-one predictions\n",
        "\t\t# in the above `for` loop\n",
        "\t\tfaces = np.array(faces, dtype=\"float32\")\n",
        "\t\tpreds = maskNet.predict(faces, batch_size=32)\n",
        "\n",
        "\t# return a 2-tuple of the face locations and their corresponding\n",
        "\t# locations\n",
        "\treturn (locs, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tEstMm6GyLK"
      },
      "source": [
        "# load our serialized face detector model from disk\n",
        "prototxtPath = r\"face_detector\\deploy.prototxt\"\n",
        "weightsPath = r\"face_detector\\res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kqRkFoRFisd"
      },
      "source": [
        "\n",
        "# load the face mask detector model from disk\n",
        "maskNet = load_model(\"mask_detector.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYLNErWRFitd"
      },
      "source": [
        "# initialize the video stream\n",
        "print(\"[INFO] starting video stream...\")\n",
        "vs = VideoStream(src=0).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHzjV9GQFtVc"
      },
      "source": [
        "\n",
        "# loop over the frames from the video stream\n",
        "while True:\n",
        "\t# grab the frame from the threaded video stream and resize it\n",
        "\t# to have a maximum width of 400 pixels\n",
        "\tframe = vs.read()\n",
        "\tframe = imutils.resize(frame, width=400)\n",
        "\n",
        "\t# detect faces in the frame and determine if they are wearing a\n",
        "\t# face mask or not\n",
        "\t(locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
        "\n",
        "\t# loop over the detected face locations and their corresponding\n",
        "\t# locations\n",
        "\tfor (box, pred) in zip(locs, preds):\n",
        "\t\t# unpack the bounding box and predictions\n",
        "\t\t(startX, startY, endX, endY) = box\n",
        "\t\t(mask, withoutMask) = pred\n",
        "\n",
        "\t\t# determine the class label and color we'll use to draw\n",
        "\t\t# the bounding box and text\n",
        "\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "\n",
        "\t\t# include the probability in the label\n",
        "\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t\t# display the label and bounding box rectangle on the output\n",
        "\t\t# frame\n",
        "\t\tcv2.putText(frame, label, (startX, startY - 10),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "\t# show the output frame\n",
        "\tcv2.imshow(\"Frame\", frame)\n",
        "\tkey = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "\t# if the `q` key was pressed, break from the loop\n",
        "\tif key == ord(\"q\"):\n",
        "\t\tbreak"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKZjfHDkFvLY"
      },
      "source": [
        "cv2.destroyAllWindows()\n",
        "vs.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}